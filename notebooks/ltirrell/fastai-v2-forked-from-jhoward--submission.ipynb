{"cells":[{"metadata":{},"cell_type":"markdown","source":"In the notebook \"[Cleaning the data for rapid prototyping](https://www.kaggle.com/jhoward/cleaning-the-data-for-rapid-prototyping-fastai)\" I showed how to create a small, fast, ready-to-use dataset for prototyping our models. The dataset created in that notebook, along with the metadata files it uses, are now [available here](https://www.kaggle.com/jhoward/rsna-hemorrhage-tif).\n\nSo let's use them to create a model! In this notebook we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition.\n\nIn my testing overnight with this notebook on my local machine I was seeing scores that would land towards the top of the leaderboard with a single model, with just some minor tweaking. I'm intentionally not doing any tricky modeling in this notebook, because I want to show the power of simple techniques and simples architectures. You should take this as a starting point and experiment! e.g. try data augmentation methods, architectures, preprocessing approaches, using the DICOM metadata, and so forth...\n\nWe'll be using the fastai.medical.imaging library here - for more information about this see the notebook [Some DICOM gotchas to be aware of](https://www.kaggle.com/jhoward/some-dicom-gotchas-to-be-aware-of-fastai). We'll also use the same basic setup that's in the notebook."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"!pip install torch torchvision feather-format kornia pyarrow --upgrade   > /dev/null\n!pip install git+https://github.com/fastai/fastai_dev                    > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from fastai2.basics           import *\nfrom fastai2.vision.all       import *\nfrom fastai2.medical.imaging  import *\nfrom fastai2.callback.tracker import *\n\nnp.set_printoptions(linewidth=120)\nmatplotlib.rcParams['image.cmap'] = 'bone'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we read in the metadata files (linked in the introduction)."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"path = Path('../input/rsna-intracranial-hemorrhage-detection/')\npath_trn = path/'stage_1_train_images'\npath_tst = path/'stage_1_test_images'\n\npath_inp = Path('../input')\npath_xtra = path_inp/'rsna-hemorrhage-tif'\npath_meta = path_xtra/'meta'/'meta'\npath_tif = path_xtra/'train_tif'/'train_tif'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"df_comb = pd.read_feather(path_meta/'comb.fth')\ndf_tst  = pd.read_feather(path_meta/'df_tst.fth').set_index('SOPInstanceUID')\ndf_samp = pd.read_feather(path_meta/'wgt_sample.fth').set_index('SOPInstanceUID')\nbins = (path_meta/'bins.pkl').load()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare sample DataBunch"},{"metadata":{},"cell_type":"markdown","source":"Our first task is to create a `DataBunch` that contains our sample data. We'll need a function to convert a filename (pointing at a DICOM file) into a path to our sample TIFF files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tif_fn(fn): return path_tif/(os.path.splitext(os.path.basename(fn))[0]+'.tif')\nfns = L(tif_fn(o) for o in df_samp.fname.values)\nfn = fns[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to be able to grab the labels from this, which we can do by simply indexing into our sample `DataFrame`."},{"metadata":{"trusted":true},"cell_type":"code","source":"htypes = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\ndef fn2label(fn): return df_samp.loc[Path(fn).with_suffix('').name][htypes].values.astype(np.float32)\nfn2label(fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you have a larger GPU or more workers, change batchsize and number-of-workers here:"},{"metadata":{"trusted":true},"cell_type":"code","source":"bs,nw = 128,4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to use fastai's new [Transform Pipeline API](http://dev.fast.ai/pets.tutorial.html) to create the DataBunch, since this is extremely flexible, which is great for intermediate and advanced Kagglers. (Beginners will probably want to stick with the Data Blocks API). We create two transform pipelines, one to open the TIFF image file, and one to look up the label and create a tensor of categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = [[PILCTScan.create], [fn2label,EncodedMultiCategorize(htypes)]]\nsplit_idx = RandomSplitter()(fns)\ndsrc = DataSource(fns, tfms, splits=split_idx)\n# I took the mean/std of a single batch to get these approximate values\nnrm = Normalize(tensor([0.6]),tensor([0.25]))\n# Since we have 16 bit data we have to divide by 2**16 to convert to the range (0,1)\nbatch_tfms = [IntToFloatTensor(div=2.**16), nrm, Cuda()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To support progressive resizing (one of the most useful tricks in the deep learning practitioner's toolbox!) we create a function that returns a dataset resized to a requested size:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def get_data(bs, sz):\n    return dsrc.databunch(bs=bs, num_workers=nw, after_item=[ToTensor],\n                          after_batch=batch_tfms+[AffineCoordTfm(size=sz)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try it out!"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dbch = get_data(128, 96)\n# xb,yb = to_cpu(dbch.one_batch())  # not running during commit\n# dbch.show_batch(max_n=4, figsize=(9,6)) # not running during commit\n# xb.mean(),xb.std(),xb.shape # not running during commit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's track the accuracy of the *any* label as our main metric, since it's easy to interpret."},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_any(inp, targ, thresh=0.5, sigmoid=True):\n    inp,targ = flatten_check(inp[:,0],targ[:,0])\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The loss function in this competition is weighted, so let's train using that loss function too."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_weights = tensor(2.0, 1, 1, 1, 1, 1).cuda()\nloss_func = BaseLoss(nn.BCEWithLogitsLoss, pos_weight=loss_weights, floatify=True, flatten=False, \n    is_2d=False, activation=torch.sigmoid)\nopt_func = partial(Adam, wd=0.01, eps=1e-3)\nmetrics=[accuracy_multi, accuracy_any]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's a handy trick: we can use a pretrained model (resnet34, in this case), and convert it to 1-channel, by simply taking the average of the initial convolutional layer!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_learner():\n    learn = cnn_learner(dbch, resnet34, loss_func=loss_func, opt_func=opt_func, metrics=metrics)\n    w = learn.model[0][0].weight\n    w.data = w.mean(1,keepdim=True)\n    # This is all that's needed to use mixed precision training in fastai :)\n    return learn.to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = get_learner()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Leslie Smith's famous LR finder will give us a reasonable learning rate suggestion."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lrf = learn.lr_find() # not running during commit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pretrain on sample"},{"metadata":{},"cell_type":"markdown","source":"Here's our main routine for changing the size of the images in our DataBunch, doing one fine-tuning of the final layers, and then training the whole model for a few epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can add additional callbacks here as needed\ncbs = []\n\ndef do_fit(bs,sz,epochs,lr):\n    learn.dbunch = get_data(bs, sz)\n    learn.opt.clear_state()\n    learn.freeze()\n    learn.fit_one_cycle(1, slice(lr), cbs=cbs)\n    learn.unfreeze()\n    learn.fit_one_cycle(epochs, slice(lr), cbs=cbs)\n    return learn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can train at 3 different sizes. Change the number of epochs below to something other than `1` - I'm just using one epoch here for demonstration, since I'm nearly out of GPU hours this week."},{"metadata":{"trusted":true},"cell_type":"code","source":"# #@# saved in previous run and using from uploaded dataset\n\n# learn = do_fit(128, 96, 2, 1e-2)\n# learn.save('s1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #@# saved in previous run and using from uploaded dataset\n\n# learn = do_fit(128, 160, 2, 3e-3)\n# learn.save('s2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #@# saved in previous run and using from uploaded dataset\n\n# learn = do_fit(128, 256, 2, 1e-3)\n# learn.save('s3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #@# saved in previous run and using from uploaded dataset\n\n# learn = do_fit(128, 384, 3, 1e-3)\n# learn.save('s4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scale up to full dataset"},{"metadata":{},"cell_type":"markdown","source":"Now let's fine tune this model on the full dataset. We'll need all the filenames now, not just the sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"fns = df_comb.fname.values\nfn = fns[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get better validation measures, we should split on patients, not just on studies, since that's how the test set is created."},{"metadata":{"trusted":true},"cell_type":"code","source":"patients = df_comb.PatientID.unique()\npat_mask = np.random.random(len(patients))<0.8\npat_trn = patients[pat_mask]\nlen(pat_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn = df_comb[ df_comb.PatientID.isin(pat_trn)]\ndf_val = df_comb[~df_comb.PatientID.isin(pat_trn)]\nlen(df_trn),len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_comb.set_index('SOPInstanceUID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These functions are copied nearly verbatim from our [earlier cleanup notebook](https://www.kaggle.com/jhoward/cleaning-the-data-for-rapid-prototyping-fastai), so have a look there for details."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_pxrepr(dcm):\n    if dcm.PixelRepresentation != 0 or dcm.RescaleIntercept<-100: return\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n    \ndef dcm_tfm(fn): \n    try:\n        x = fn.dcmread()\n        fix_pxrepr(x)\n    except Exception as e:\n        print(fn,e)\n        raise SkipItemException\n    if x.Rows != 512 or x.Columns != 512: x.zoom_to((512,512))\n    return TensorCTScan(x.hist_scaled(bins)[None])\n\ndef fn2label(fn): return df_comb.loc[Path(fn).with_suffix('').name][htypes].values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our DataBunch creation is nearly identical to before, but uses our cleanup function and patient-level training/validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_fns = [path_trn/Path(fn).name for fn in fns]\ntfms = [[dcm_tfm], [fn2label,EncodedMultiCategorize(htypes)]]\nsplit_idx = [df_trn.index.values,df_val.index.values]\ndsrc = DataSource(trn_fns, tfms, splits=split_idx)\nbatch_tfms = [nrm, Cuda()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function is nearly the same as before, except that if we're not doing any resizing, then we skip that transform entirely."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def get_data(bs, sz=None):\n    bt = batch_tfms\n    if sz is not None: bt = bt+[AffineCoordTfm(size=sz)]\n    return dsrc.databunch(bs=bs, num_workers=nw, after_batch=bt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can test it out:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbch = get_data(128, 384)\n# x,y = to_cpu(dbch.one_batch())  # not running during commit\n# x.shape  # not running during commit\n# dbch.show_batch(max_n=4)  # not running during commit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good! Let's pop it in our learner, and we're ready to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.dbunch = dbch\nlearn.opt.clear_state()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For fine-tuning the final layers, we don't really need to use a whole epoch, so we'll use the `ShortEpochCallback` to just train for 10% of an epoch, before then unfreezing the model and training a bit more.\n\nI've commented out the final fine-tuning here since I'm nearly out of GPU time for this week!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #@# saved in previous run and using from uploaded dataset\n\n# lr = 1e-3\n# learn.freeze()\n# learn.fit_one_cycle(1, slice(lr), cbs=[ShortEpochCallback(pct=0.1, short_valid=False)])\n# learn.save('dcm-384-0')\n# learn.unfreeze()\n# learn.fit_one_cycle(1, slice(lr))\n# learn.save('dcm-384-1')\n# learn.fit_one_cycle(1, slice(lr))\n# learn.save('dcm-384-2')\n# learn.fit_one_cycle(1, slice(lr))\n# learn.save('dcm-384-3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally train a little more on the full size."},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p models && cp ../input/rsnaintracranialhemorrhagedetectionfastaiv2/dcm-384-2.pth models\nlearn.load('dcm-384-2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndbch = get_data(128, None)\nlearn.dbunch = dbch\nlearn.opt.clear_state()\nlr = 1e-4\nlearn.freeze()\nlearn.fit_one_cycle(1, slice(lr), cbs=[ShortEpochCallback(pct=0.05, short_valid=False)])\nlearn.save('dcm-full-0')\nlearn.unfreeze()\nlearn.fit_one_cycle(1, slice(lr))\nlearn.save('dcm-full-1')\nlearn.fit_one_cycle(1, slice(lr))\nlearn.save('dcm-full-2')\n# learn.fit_one_cycle(1, slice(lr))\n# learn.save('dcm-full-3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare for submission"},{"metadata":{},"cell_type":"markdown","source":"Now we're ready to submit. We can use the handy `test_dl` function to get an inference `DataLoader` ready, then we can check it looks OK."},{"metadata":{"trusted":true},"cell_type":"code","source":"fns = df_tst.fname.values\ntst_fns = [path_tst/Path(fn).name for fn in fns]\n\ntst = test_dl(dbch, tst_fns)\nx = tst.one_batch()[0]\nx.min(),x.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We pass that to `get_preds` to get our predictions, and then clamp them just in case we have some extreme values."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,targs = learn.get_preds(dl=tst)\npreds_clipped = preds.clamp(.00001, .9999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm too lazy to write a function that creates a submission file, so this code is stolen from Radek, with minor changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = []\nlabels = []\n\nfor idx,pred in zip(df_tst.index, preds_clipped):\n    for i,label in enumerate(htypes):\n        ids.append(f\"{idx}_{label}\")\n        predicted_probability = '{0:1.10f}'.format(pred[i].item())\n        labels.append(predicted_probability)\n\ndf_csv = pd.DataFrame({'ID': ids, 'Label': labels})\ndf_csv.to_csv(f'submission.csv', index=False)\ndf_csv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the code below if you want a link to download the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink, FileLinks\nFileLink('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}